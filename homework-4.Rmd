---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Jason Carlson"
date: "2/28/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r include=FALSE}
library(e1071)
library(gbm)
library(MASS)
library(tidyverse)
library(broom)
library(caret)
library(ISLR)
library(janitor)
library(plotROC)
library(kernlab)
library(glmnet)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit) 
library(tree)
library(randomForest)
theme_set(theme_bw())
```

## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1
```{r}
set.seed(1234)
df <- tbl_df(Boston)

for (k in 1:20){
  inTraining <- createDataPartition(df$medv, p = .75, list = F)
  training <- df[inTraining, ]
  testing <- df[-inTraining, ]
  mtry <- c(3:9)
  ntree <- seq(25, 500, len = 20)
  results <- tibble(trial = rep(NA, 140),
  mtry = rep(NA, 140),
  ntree = rep(NA, 140),
  mse = rep(NA, 140)) 
  for(i in 1:7){
    cat(sprintf('Trial: %s, mtry: %s --- %s\n', k, mtry[i], Sys.time()))
    for(j in 1:20){ 
      rf_train <- randomForest(medv ~ .,
                               data = training,
                               mtry = mtry[i],
                               ntree = ntree[j])
      mse <- mean((predict(rf_train, newdata = testing) - testing$medv)^2)
      results[(i-1)*20 + j, ] <- c(k, mtry[i], ntree[j], mse)
    }
  }
  if(exists("results_total")){
  results_total <- bind_rows(results_total, results)
  }
  else(
  results_total <- results
  )
}
```


## Problem 2
 


Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:
#A
```{r}
set.seed(9823)
df <- tbl_df(Carseats)
inTraining <- createDataPartition(df$Sales, p = .5, list = F)
training <- df[inTraining, ]
testing  <- df[-inTraining, ]

```
#B
```{r}
carseats_tree <- rpart(Sales ~ ., data = training)
prp(carseats_tree)

summary(carseats_tree)

#MSE=7.428204
```
#C
```{r}
fit_control <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 10)
cv_carseat_tree <- train(Sales ~ ., 
                        data = training,
                        method = "rpart2", 
                        trControl = fit_control,
                        tuneGrid = data.frame(maxdepth = 1:7))
plot(cv_carseat_tree)

test_preds <- predict(cv_carseat_tree, newdata = testing)
carseat_test_df <- testing %>%
  mutate(y_hat = test_preds,
         sq_err = (y_hat - Sales)^2)
mean(carseat_test_df$sq_err)

#MSE: 4.933184
#Cross Validation improved MSE
```
#D
```{r}
bag_carseat <- randomForest(Sales ~ ., data = training, mtry = 10, importance=TRUE)

bag_preds <- predict(bag_carseat, newdata = testing)
mean((bag_preds - testing$Sales)^2)
#MSE: 3.073112

importance(bag_carseat)

#ShelveLoc and Price are the most important variables (highest %IncMSE)
```
#E
```{r}
mseDF<-NA
for (i in 1:10){
  rforest_carseats <-  randomForest(Sales ~ . , data=training, 
                             mtry=i, ntree=500, importance=TRUE)
  rforest_pred <-  predict(rforest_carseats, testing)
  mseDF[i] <- mean((testing$Sales - rforest_pred)^2)
}

bestM=which.min(mseDF)
bestM
mseDF[bestM]
best_rforest_carseats <-  randomForest(Sales ~ . , data=training, 
                             mtry=bestM, ntree=500, importance=TRUE)
importance(best_rforest_carseats)
#ShelveLoc and Price are still the two most important variables

#The best M varies between 7-10 (for the times I have ran the loop to find it. The best MSE I found was using M=9 resulting in MSE of 2.99
```



1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
```{r}
carseats.boost<-gbm(Sales ~ . ,data = training,distribution = "gaussian",n.trees = 10000,
                  shrinkage = 0.01, interaction.depth = 4)
carseats_boost_tree <- rpart::rpart(Sales ~ . , 
                      data = training,
                      control = rpart.control(minsplit = 50))

boost_preds <- predict(carseats_boost_tree, newdata = testing)
mean((boost_preds - testing$Sales)^2)

#5.428695
```




2. Fit a multiple regression model to the training data and report the 
estimated test MSE
```{r}
fit_control <- trainControl(method = "repeatedcv",
                          number = 10, 
                           repeats = 10)


carseat_mult <- train(Sales ~ .,
                          data = training,
                          method = "svmPoly",
                          trControl = fit_control,
                         tuneGrid = expand.grid(degree = 2:4,
                                                scale = c(.001, .01, .1), 
                                                 C = 2:8))
carseat_mult_preds<-predict(carseat_mult, newdata = testing)
```



3. Summarize your results. 
```{r}
mean((carseat_mult_preds - testing$Sales)^2)

#MSE: 1.104752
#The multiple regression model is by far the best out of all models as it has the lowest MSE (1.104752)
```

